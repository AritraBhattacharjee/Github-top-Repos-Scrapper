{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21c7bcf6",
   "metadata": {},
   "source": [
    "# Github Topic and Repositories Scrapper\n",
    "    \n",
    "   ### Objective:\n",
    "         - To get the Github page of topics, url : \"www.github.com/topics\"\n",
    "         - Parse the downloaded html content using Beautiful Soup\n",
    "         - Get the desired contents/list of contents from the soup object.\n",
    "         - Make a DataFrame using pandas libraries of the scraped data.\n",
    "         - Finally save the dataframe as .csv or .xlsx according to our preference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437e7c2",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f98ff9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests,urllib\n",
    "import html5lib\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b903f9e2",
   "metadata": {},
   "source": [
    "### As we are Scraping Github topics page, we are assigning the url to a variable named topics_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95edef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_url = 'https://www.github.com/topics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6f7693",
   "metadata": {},
   "source": [
    "**We are sending a get request using the python requests library and we are getting the html page as the response to the request sent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ed8f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7387f",
   "metadata": {},
   "source": [
    "**We are checking the status code to ensure we have successfully got the webpage response. Status code 200 means that the request was successful. To know more about HTTP Status code, you can refer to [MDN References](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status) of HTTP response status codes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e52f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe27c64",
   "metadata": {},
   "source": [
    "**Getting the HTML content from the downloaded page.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d7f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagecontent = response.text\n",
    "# len(pagecontent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2142b402",
   "metadata": {},
   "source": [
    "**Parsing the HTML Content recieved from the get requests, using the Beautiful Soup Library.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e048335f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(pagecontent,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a04f5c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27f25a",
   "metadata": {},
   "source": [
    "**We can modify the apperance of the HTML content that we parsed, we can use a function called *Prettify()*.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15392d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62994ea1",
   "metadata": {},
   "source": [
    "**We made a function *parse_star_count(star_str)* that parses the star count(passed as a string argument to the function) into a more readable format.  \n",
    "For example:  \n",
    "80.3K will be written/parsed as 80300**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ebb1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(star_str):\n",
    "    star_str = star_str.strip()\n",
    "    if star_str[-1] == 'k':\n",
    "        return int(float(star_str[:-1])*1000)\n",
    "    return int(star_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b447788",
   "metadata": {},
   "source": [
    "**Utility function _get_repo_info(h3_tag,star_tag)_ that seperates username, reponame, repo_url and stars count of the repository from the h3 tag list and star_tag list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cf82bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_repo_info(h3_tag,star_tag):\n",
    "    atags = h3_tag.find_all('a')\n",
    "    username = atags[0].text.strip()\n",
    "    reponame = atags[1].text.strip()\n",
    "    repo_url = \"https://www.github.com\"+ atags[1]['href']\n",
    "    stars = parse_star_count(star_tag.text.strip())\n",
    "    \n",
    "    return username,reponame,repo_url,stars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b65d7f",
   "metadata": {},
   "source": [
    "**Function get_topic_page(topic_url) : It takes the topic url as an argument and then fetches the content using the get request of ___requests___ library and then parse the content using BeautifulSoup and returns the same parsed object.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea737a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_page(topic_url):\n",
    "    response = requests.get(topic_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))\n",
    "        \n",
    "    topic_doc = BeautifulSoup(response.text,'html.parser')\n",
    "    return topic_doc "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602177ee",
   "metadata": {},
   "source": [
    "**Function _get_topic_repos(topic_doc)_: It takes a Beautiful Soup Object as an argument and find all the h3 tags that contains information about the User name, Repository Name, Repo Url, Stars, stores them in a dictionary and return a object created by converting the dictionary to a Pandas DataFrame**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8aa0088",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_topic_repos(topic_doc):\n",
    "    \n",
    "    repo_tags = topic_doc.find_all('h3',attrs={\n",
    "        'class' : 'f3 color-fg-muted text-normal lh-condensed'})\n",
    "    stars_tags= topic_doc.find_all('span',attrs= {\n",
    "        'class' : 'Counter js-social-count'\n",
    "            })\n",
    "    topic_repo_dicts = {\n",
    "    'Username':[],\n",
    "    'Repo_Name':[],\n",
    "    'Repo_Url':[],\n",
    "    'Stars': []\n",
    "    }\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info = get_repo_info(repo_tags[i],stars_tags[i])\n",
    "        topic_repo_dicts['Username'].append(repo_info[0])\n",
    "        topic_repo_dicts['Repo_Name'].append(repo_info[1])\n",
    "        topic_repo_dicts['Repo_Url'].append(repo_info[2])\n",
    "        topic_repo_dicts['Stars'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repo_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "806f06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_topic(topic_url,topic_name):\n",
    "    filename = topic_name+\".csv\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"File {filename} already exists. Skipping...\")\n",
    "        return \n",
    "    topic_df = get_topic_repos(get_topic_page(topic_url))\n",
    "    \n",
    "    topic_df.to_csv(filename,index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05a54785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(soup):\n",
    "    topic_title_tags = soup.find_all('p',attrs={\n",
    "    'class':'f3 lh-condensed mb-0 mt-1 Link--primary'\n",
    "    })\n",
    "    topic_link_tag = soup.find_all('a',{'class' : 'no-underline flex-grow-0'})\n",
    "    topic_titles = []\n",
    "    \n",
    "    for tag in topic_title_tags:\n",
    "        topic_titles.append(tag.text)\n",
    "    return topic_titles\n",
    "\n",
    "def get_topic_description(soup):\n",
    "    topic_desc_tag = soup.find_all('p', attrs=\n",
    "                                   {'class':'f5 color-fg-muted mb-0 mt-1'})\n",
    "    topic_descriptions = []\n",
    "\n",
    "    for tag in topic_desc_tag:\n",
    "        topic_descriptions.append(tag.text.strip())\n",
    "    return topic_descriptions\n",
    "\n",
    "def get_topic_urls(soup):\n",
    "    topic_link_tags = soup.find_all('a',attrs = \n",
    "                            {'class' : 'no-underline flex-grow-0'})\n",
    "    topic_urls = []\n",
    "    base = \"https://www.github.com\"\n",
    "    for tag in topic_link_tags:\n",
    "        topic_urls.append(base+tag['href'])\n",
    "    return topic_urls\n",
    "\n",
    "def scrape_topics():\n",
    "    topics_url = \"https://github.com/topics\"\n",
    "#     topics_url = \"https://github.com/topics?page=1\"\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code !=200:\n",
    "        raise Exception('Failed to load page {}'.format(topic_url))  \n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    topics_dict = {\n",
    "        'title' : get_topic_titles(soup),\n",
    "        'decscription': get_topic_description(soup),\n",
    "        'url':get_topic_urls(soup)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89ae1ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics_repos():\n",
    "    print(\"Scraping list topics from Github\")\n",
    "    topics_df = scrape_topics()\n",
    "    for index,row in topics_df.iterrows():\n",
    "        print(f\"Scraping top repositories for {row['title']}\")\n",
    "        scrape_topic(row['url'],row['title'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deb758d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping list topics from Github\n",
      "Scraping top repositories for 3D\n",
      "File 3D.csv already exists. Skipping...\n",
      "Scraping top repositories for Ajax\n",
      "File Ajax.csv already exists. Skipping...\n",
      "Scraping top repositories for Algorithm\n",
      "File Algorithm.csv already exists. Skipping...\n",
      "Scraping top repositories for Amp\n",
      "File Amp.csv already exists. Skipping...\n",
      "Scraping top repositories for Android\n",
      "File Android.csv already exists. Skipping...\n",
      "Scraping top repositories for Angular\n",
      "File Angular.csv already exists. Skipping...\n",
      "Scraping top repositories for Ansible\n",
      "File Ansible.csv already exists. Skipping...\n",
      "Scraping top repositories for API\n",
      "File API.csv already exists. Skipping...\n",
      "Scraping top repositories for Arduino\n",
      "File Arduino.csv already exists. Skipping...\n",
      "Scraping top repositories for ASP.NET\n",
      "File ASP.NET.csv already exists. Skipping...\n",
      "Scraping top repositories for Atom\n",
      "File Atom.csv already exists. Skipping...\n",
      "Scraping top repositories for Awesome Lists\n",
      "File Awesome Lists.csv already exists. Skipping...\n",
      "Scraping top repositories for Amazon Web Services\n",
      "File Amazon Web Services.csv already exists. Skipping...\n",
      "Scraping top repositories for Azure\n",
      "File Azure.csv already exists. Skipping...\n",
      "Scraping top repositories for Babel\n",
      "File Babel.csv already exists. Skipping...\n",
      "Scraping top repositories for Bash\n",
      "File Bash.csv already exists. Skipping...\n",
      "Scraping top repositories for Bitcoin\n",
      "File Bitcoin.csv already exists. Skipping...\n",
      "Scraping top repositories for Bootstrap\n",
      "File Bootstrap.csv already exists. Skipping...\n",
      "Scraping top repositories for Bot\n",
      "File Bot.csv already exists. Skipping...\n",
      "Scraping top repositories for C\n",
      "File C.csv already exists. Skipping...\n",
      "Scraping top repositories for Chrome\n",
      "File Chrome.csv already exists. Skipping...\n",
      "Scraping top repositories for Chrome extension\n",
      "File Chrome extension.csv already exists. Skipping...\n",
      "Scraping top repositories for Command line interface\n",
      "File Command line interface.csv already exists. Skipping...\n",
      "Scraping top repositories for Clojure\n",
      "File Clojure.csv already exists. Skipping...\n",
      "Scraping top repositories for Code quality\n",
      "File Code quality.csv already exists. Skipping...\n",
      "Scraping top repositories for Code review\n",
      "File Code review.csv already exists. Skipping...\n",
      "Scraping top repositories for Compiler\n",
      "File Compiler.csv already exists. Skipping...\n",
      "Scraping top repositories for Continuous integration\n",
      "File Continuous integration.csv already exists. Skipping...\n",
      "Scraping top repositories for COVID-19\n",
      "File COVID-19.csv already exists. Skipping...\n",
      "Scraping top repositories for C++\n",
      "File C++.csv already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "scrape_topics_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b51a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  @aevdcxegbvfcx wdvcxvfsgerfwdeasxzxcfdgrfeadsxzcfrwfeadsxcvfrgfdascxzcvvfwfedasxzcwgfeadsczxvwfeadscxdwfeasdxz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e4c16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
